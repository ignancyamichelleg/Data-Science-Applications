{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqSjHsuRPOKGe+KgPRWJkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ignancyamichelleg/Data-Science-Applications/blob/main/TextDataPartitioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lrqef6qyxvSg"
      },
      "outputs": [],
      "source": [
        "import re                  # Regular expressions for text cleaning\n",
        "import random              # For random sampling\n",
        "import requests            # For downloading books from the internet\n",
        "import pandas as pd        # For organizing data in tables\n",
        "import pickle              # For saving Python objects\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_book(book_id, book_label):\n",
        "    \"\"\"\n",
        "    Download a book from Project Gutenberg.\n",
        "\n",
        "    Parameters:\n",
        "    - book_id: The ID number of the book on Project Gutenberg\n",
        "    - book_label: A single letter label ('a', 'b', or 'c')\n",
        "\n",
        "    Returns:\n",
        "    - The text of the book as a string, or None if download fails\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n--- Downloading Book {book_label.upper()} (ID: {book_id}) ---\")\n",
        "\n",
        "    # Try the first URL format\n",
        "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "\n",
        "        # If first URL doesn't work, try second format\n",
        "        if response.status_code != 200:\n",
        "            url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}.txt\"\n",
        "            response = requests.get(url, timeout=30)\n",
        "\n",
        "        # Check if download was successful\n",
        "        if response.status_code == 200:\n",
        "            print(f\"Successfully downloaded! Size: {len(response.text)} characters\")\n",
        "            return response.text\n",
        "        else:\n",
        "            print(f\"Failed to download (Status code: {response.status_code})\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during download: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "D6rSy383yFMk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean the book text by removing headers, footers, and extra spaces.\n",
        "\n",
        "    This uses Regular Expressions to:\n",
        "    - Remove Project Gutenberg's legal header\n",
        "    - Remove Project Gutenberg's legal footer\n",
        "    - Remove extra whitespace\n",
        "\n",
        "    Parameters:\n",
        "    - text: The raw text from the book\n",
        "\n",
        "    Returns:\n",
        "    - Cleaned text ready for processing\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"  Cleaning text...\")\n",
        "\n",
        "    # REGEX 1: Remove everything before \"*** START OF\"\n",
        "    # This removes the legal header that Project Gutenberg adds\n",
        "    start_pattern = r'\\*\\*\\*.*?START OF.*?\\*\\*\\*'\n",
        "    parts = re.split(start_pattern, text, flags=re.IGNORECASE)\n",
        "    if len(parts) > 1:\n",
        "        text = parts[-1]  # Keep everything after the start marker\n",
        "\n",
        "    # REGEX 2: Remove everything after \"*** END OF\"\n",
        "    # This removes the legal footer that Project Gutenberg adds\n",
        "    end_pattern = r'\\*\\*\\*.*?END OF.*?\\*\\*\\*'\n",
        "    parts = re.split(end_pattern, text, flags=re.IGNORECASE)\n",
        "    text = parts[0]  # Keep everything before the end marker\n",
        "\n",
        "    # REGEX 3: Replace multiple spaces, tabs, and newlines with single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # REGEX 4: Remove any asterisks\n",
        "    text = re.sub(r'\\*+', '', text)\n",
        "\n",
        "    # Remove leading and trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    print(f\"Text cleaned! Length: {len(text)} characters\")\n",
        "    return text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: EXTRACT WORDS USING REGULAR EXPRESSIONS\n",
        "# ============================================================================\n",
        "\n",
        "def extract_words(text):\n",
        "    \"\"\"\n",
        "    Extract individual words from the text using Regular Expressions.\n",
        "\n",
        "    A word is defined as letters and apostrophes (for contractions like \"don't\").\n",
        "\n",
        "    Parameters:\n",
        "    - text: Cleaned text\n",
        "\n",
        "    Returns:\n",
        "    - List of words\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"  Extracting words...\")\n",
        "\n",
        "    # REGEX 5: Match words (letters and apostrophes for contractions)\n",
        "    # \\b = word boundary\n",
        "    # [A-Za-z]+ = one or more letters\n",
        "    # (?:'[A-Za-z]+)? = optionally an apostrophe followed by more letters\n",
        "    word_pattern = r\"\\b[A-Za-z]+(?:'[A-Za-z]+)?\\b\"\n",
        "\n",
        "    words = re.findall(word_pattern, text)\n",
        "\n",
        "    print(f\"Extracted {len(words)} words\")\n",
        "    return words"
      ],
      "metadata": {
        "id": "wyLES5Z9ysqL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_partitions(words, book_label, num_partitions=200, words_per_partition=100):\n",
        "    \"\"\"\n",
        "    Create random 100-word partitions from the list of words.\n",
        "\n",
        "    Parameters:\n",
        "    - words: List of all words from the book\n",
        "    - book_label: Label for this book ('a', 'b', or 'c')\n",
        "    - num_partitions: How many partitions to create (default: 200)\n",
        "    - words_per_partition: Words in each partition (default: 100)\n",
        "\n",
        "    Returns:\n",
        "    - List of dictionaries, each containing one partition\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"  Creating {num_partitions} random partitions of {words_per_partition} words each...\")\n",
        "\n",
        "    total_words = len(words)\n",
        "    partitions = []\n",
        "\n",
        "    # Make sure we have enough words\n",
        "    if total_words < words_per_partition:\n",
        "        print(f\"Not enough words! Need {words_per_partition}, only have {total_words}\")\n",
        "        return partitions\n",
        "\n",
        "    # Create the requested number of partitions\n",
        "    for i in range(num_partitions):\n",
        "        # Pick a random starting position\n",
        "        # We need to leave room for 100 words, so max start is total_words - 100\n",
        "        max_start = total_words - words_per_partition\n",
        "        start_position = random.randint(0, max_start)\n",
        "\n",
        "        # Extract 100 words starting from that position\n",
        "        partition_words = words[start_position:start_position + words_per_partition]\n",
        "\n",
        "        # Join the words back into a text string\n",
        "        partition_text = ' '.join(partition_words)\n",
        "\n",
        "        # Create a record for this partition\n",
        "        partition_record = {\n",
        "            'partition_id': f\"{book_label}_{i+1}\",  # Example: \"a_1\", \"a_2\", etc.\n",
        "            'book_label': book_label,\n",
        "            'partition_number': i + 1,\n",
        "            'text': partition_text,\n",
        "            'word_count': len(partition_words)\n",
        "        }\n",
        "\n",
        "        partitions.append(partition_record)\n",
        "\n",
        "    print(f\"Created {len(partitions)} partitions\")\n",
        "    return partitions\n"
      ],
      "metadata": {
        "id": "PVjDM3h9yy74"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_book(book_id, book_label, num_partitions=200):\n",
        "    \"\"\"\n",
        "    Process one book: download, clean, extract words, create partitions.\n",
        "\n",
        "    Parameters:\n",
        "    - book_id: Project Gutenberg book ID\n",
        "    - book_label: Single letter label\n",
        "    - num_partitions: Number of 100-word partitions to create\n",
        "\n",
        "    Returns:\n",
        "    - List of partition records for this book\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Download\n",
        "    text = download_book(book_id, book_label)\n",
        "    if text is None:\n",
        "        return []\n",
        "\n",
        "    # Step 2: Clean\n",
        "    clean = clean_text(text)\n",
        "\n",
        "    # Step 3: Extract words\n",
        "    words = extract_words(clean)\n",
        "\n",
        "    # Step 4: Create partitions\n",
        "    partitions = create_partitions(words, book_label, num_partitions)\n",
        "\n",
        "    return partitions"
      ],
      "metadata": {
        "id": "kx1ZS5HLy43S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(dataframe, output_folder):\n",
        "    \"\"\"\n",
        "    Save the data in three different formats: CSV, JSON, and Pickle.\n",
        "\n",
        "    Parameters:\n",
        "    - dataframe: Pandas DataFrame containing all partitions\n",
        "    - output_folder: Folder to save the files\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n--- Saving Data ---\")\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    Path(output_folder).mkdir(exist_ok=True)\n",
        "\n",
        "    # Save as CSV (comma-separated values - can open in Excel)\n",
        "    csv_path = f\"{output_folder}/partitions.csv\"\n",
        "    dataframe.to_csv(csv_path, index=False)\n",
        "    print(f\"Saved CSV file: {csv_path}\")\n",
        "\n",
        "    # Save as JSON (JavaScript Object Notation - human readable)\n",
        "    json_path = f\"{output_folder}/partitions.json\"\n",
        "    dataframe.to_json(json_path, orient='records', indent=2)\n",
        "    print(f\"Saved JSON file: {json_path}\")\n",
        "\n",
        "    # Save as Pickle (Python binary format - preserves exact data types)\n",
        "    pickle_path = f\"{output_folder}/partitions.pkl\"\n",
        "    dataframe.to_pickle(pickle_path)\n",
        "    print(f\"Saved Pickle file: {pickle_path}\")\n",
        "\n",
        "    # Also save a summary statistics file\n",
        "    summary_path = f\"{output_folder}/summary_statistics.txt\"\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(\"SUMMARY STATISTICS\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Total partitions: {len(dataframe)}\\n\")\n",
        "        f.write(f\"Books processed: {dataframe['book_label'].nunique()}\\n\\n\")\n",
        "        f.write(\"Partitions per book:\\n\")\n",
        "        f.write(str(dataframe['book_label'].value_counts().sort_index()))\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"Sample of first 5 rows:\\n\")\n",
        "        f.write(str(dataframe.head()))\n",
        "    print(f\"Saved summary: {summary_path}\")\n"
      ],
      "metadata": {
        "id": "Wn8YZvq8y9Rh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function that runs the entire program.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"GUTENBERG BOOK PROCESSOR\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CONFIGURATION - Change these settings as needed\n",
        "    # ========================================================================\n",
        "\n",
        "    # Three popular books from Project Gutenberg\n",
        "    books = [\n",
        "        {'id': 1342, 'label': 'a', 'title': 'Pride and Prejudice by Jane Austen'},\n",
        "        {'id': 11, 'label': 'b', 'title': 'Alice\\'s Adventures in Wonderland by Lewis Carroll'},\n",
        "        {'id': 84, 'label': 'c', 'title': 'Frankenstein by Mary Shelley'}\n",
        "    ]\n",
        "\n",
        "    num_partitions = 200      # Number of partitions per book\n",
        "    output_folder = \"output\"  # Where to save results\n",
        "\n",
        "    # ========================================================================\n",
        "    # PROCESS EACH BOOK\n",
        "    # ========================================================================\n",
        "\n",
        "    all_partitions = []  # This will hold all partitions from all books\n",
        "\n",
        "    for book in books:\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"Processing: {book['title']}\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "\n",
        "        # Process this book\n",
        "        partitions = process_book(book['id'], book['label'], num_partitions)\n",
        "\n",
        "        # Add these partitions to our complete collection\n",
        "        all_partitions.extend(partitions)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CREATE PANDAS DATAFRAME\n",
        "    # ========================================================================\n",
        "\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(\"Creating Pandas DataFrame\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # Convert list of dictionaries to Pandas DataFrame\n",
        "    df = pd.DataFrame(all_partitions)\n",
        "\n",
        "    print(f\" Created DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
        "    print(f\"\\nColumns: {list(df.columns)}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Show statistics by book\n",
        "    print(f\"\\nPartitions per book:\")\n",
        "    print(df['book_label'].value_counts().sort_index())\n",
        "\n",
        "    # ========================================================================\n",
        "    # SAVE RESULTS\n",
        "    # ========================================================================\n",
        "\n",
        "    save_data(df, output_folder)\n",
        "\n",
        "    # ========================================================================\n",
        "    # DONE!\n",
        "    # ========================================================================\n",
        "\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(\" PROCESSING COMPLETE!\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "    print(f\"\\nAll files saved to '{output_folder}/' folder\")\n",
        "    print(f\"Total partitions created: {len(df)}\")\n",
        "    print(\"\\nYou can now:\")\n",
        "    print(\"  - Open partitions.csv in Excel or any spreadsheet program\")\n",
        "    print(\"  - View partitions.json in any text editor\")\n",
        "    print(\"  - Load partitions.pkl in another Python program\")"
      ],
      "metadata": {
        "id": "fjbAaHIey-HN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility (same random samples each time)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Run the main program\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-QN-4R5zL3A",
        "outputId": "e180227f-f45a-4db4-8970-fe504c3c151b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "GUTENBERG BOOK PROCESSOR\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Processing: Pride and Prejudice by Jane Austen\n",
            "======================================================================\n",
            "\n",
            "--- Downloading Book A (ID: 1342) ---\n",
            "Successfully downloaded! Size: 743383 characters\n",
            "  Cleaning text...\n",
            "Text cleaned! Length: 718447 characters\n",
            "  Extracting words...\n",
            "Extracted 127959 words\n",
            "  Creating 200 random partitions of 100 words each...\n",
            "Created 200 partitions\n",
            "\n",
            "======================================================================\n",
            "Processing: Alice's Adventures in Wonderland by Lewis Carroll\n",
            "======================================================================\n",
            "\n",
            "--- Downloading Book B (ID: 11) ---\n",
            "Successfully downloaded! Size: 144696 characters\n",
            "  Cleaning text...\n",
            "Text cleaned! Length: 143067 characters\n",
            "  Extracting words...\n",
            "Extracted 27175 words\n",
            "  Creating 200 random partitions of 100 words each...\n",
            "Created 200 partitions\n",
            "\n",
            "======================================================================\n",
            "Processing: Frankenstein by Mary Shelley\n",
            "======================================================================\n",
            "\n",
            "--- Downloading Book C (ID: 84) ---\n",
            "Successfully downloaded! Size: 419434 characters\n",
            "  Cleaning text...\n",
            "Text cleaned! Length: 418285 characters\n",
            "  Extracting words...\n",
            "Extracted 75194 words\n",
            "  Creating 200 random partitions of 100 words each...\n",
            "Created 200 partitions\n",
            "\n",
            "======================================================================\n",
            "Creating Pandas DataFrame\n",
            "======================================================================\n",
            " Created DataFrame with 600 rows and 5 columns\n",
            "\n",
            "Columns: ['partition_id', 'book_label', 'partition_number', 'text', 'word_count']\n",
            "\n",
            "First few rows:\n",
            "  partition_id book_label  partition_number  \\\n",
            "0          a_1          a                 1   \n",
            "1          a_2          a                 2   \n",
            "2          a_3          a                 3   \n",
            "3          a_4          a                 4   \n",
            "4          a_5          a                 5   \n",
            "\n",
            "                                                text  word_count  \n",
            "0  to the carriage as quickly as possible Her nie...         100  \n",
            "1  did not quit her room for a moment nor were th...         100  \n",
            "2  that Charlotte Lucas her egregious papa though...         100  \n",
            "3  be in debt to every tradesman in the place and...         100  \n",
            "4  composed and steady gravity At length however ...         100  \n",
            "\n",
            "Partitions per book:\n",
            "book_label\n",
            "a    200\n",
            "b    200\n",
            "c    200\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Saving Data ---\n",
            "Saved CSV file: output/partitions.csv\n",
            "Saved JSON file: output/partitions.json\n",
            "Saved Pickle file: output/partitions.pkl\n",
            "Saved summary: output/summary_statistics.txt\n",
            "\n",
            "======================================================================\n",
            " PROCESSING COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "All files saved to 'output/' folder\n",
            "Total partitions created: 600\n",
            "\n",
            "You can now:\n",
            "  - Open partitions.csv in Excel or any spreadsheet program\n",
            "  - View partitions.json in any text editor\n",
            "  - Load partitions.pkl in another Python program\n"
          ]
        }
      ]
    }
  ]
}